# EfficientNet v1 â€” Food-101 (Multiclass)

## ğŸ§  Introduction

This repository implements **EfficientNet v1** (*Tan & Le, 2019, â€œRethinking Model Scaling for Convolutional Neural Networksâ€*) from scratch in **PyTorch**, applying it to the **Food-101** dataset (101 classes, ~100k images).

EfficientNet introduced the idea of **compound scaling** â€” instead of scaling only depth or width, the network scales *resolution*, *width* and *depth* **jointly and proportionally** using three coefficients:

$$
d = \alpha^\phi, \quad w = \beta^\phi, \quad r = \gamma^\phi
$$
subject to a constraint $\alpha \cdot \beta^2 \cdot \gamma^2 \approx 2$.

This simple principle allows EfficientNet-B0 to be expanded consistently up to B7, achieving **state-of-the-art accuracy/efficiency trade-offs** across multiple datasets.

Key contributions:
- ğŸ”¹ **Compound scaling** strategy with balanced growth of all dimensions.  
- ğŸ”¹ **MBConv** inverted bottlenecks with *depthwise separable convolutions*.  
- ğŸ”¹ **Squeeze-and-Excitation (SE)** attention for adaptive channel reweighting.  
- ğŸ”¹ **Stochastic Depth (Drop-Connect)** for better regularization in deep networks.  
- ğŸ”¹ A clean, parameter-efficient design achieving strong performance with fewer FLOPs.

EfficientNet-B0 achieves **~77% top-1 accuracy on ImageNet** with only **5.3 M parameters** â€” nearly 8Ã— fewer than ResNet-152.


## ğŸ—‚ï¸ Project Structure

The repository is organized into modular, fully testable components:

### 1. `data/`
Dataset loading and preprocessing.
- `load_data.py` â€” downloads and prepares **Food-101**, resizing images to 200Ã—200.  
- `best_trainloaders.py` â€” optimized dataloaders with augmentation and caching.  
- `loaders_verification.py` â€” quick checks for dataset integrity and class balance.

### 2. `model/`
Core architecture and reusable building blocks.
- `cnn_utils.py` â€” convolutional helpers and padding logic (`ConvBNAct`).  
- `computer_scaler.py` â€” compound scaling utilities (`CompoundScaler`, `round_filters`, etc.).  
- `efficient_blocks.py` â€” **Squeeze-and-Excitation (SE)** and **Stochastic Depth** modules.  
- `MBConv.py` â€” implementation of the **MBConv** block with expand-depthwise-project stages.  
- `Efficient_Net.py` â€” full **EfficientNet v1** model (B0â€“B7) with compound scaling.  
- `train_loop.py` â€” AMP-ready training loop with gradient clipping and label smoothing.

### 3. `tests/`
Unit tests for every component.
- `test_utils_scaling.py` â€” checks scaling and divisibility logic.  
- `test_layers.py` â€” validates `ConvBNAct`, `SqueezeExcitation`, and `MBConv` behavior.  
- `test_efficientnet_shapes_and_params.py` â€” verifies forward passes and parameter growth across B0â€“B7.  
- `test_dynamic_resize_and_train_smoke.py` â€” smoke test for the `DynamicResize` layer and a mini training step.  
- `test_serialization.py` â€” ensures consistent save/load of model weights.  
- `test_param_breakdown.py` â€” parameter counting consistency tests.

### 4. `training/`
Scripts for launching experiments.
- `train_model.py` â€” complete training pipeline for Food-101.  
- `EfficientNet_full.ipynb` â€” notebook demonstrating compound scaling, stochastic depth, and results visualization.

## âš™ï¸ Technical Details

### Training Configuration
- **Optimizer:** RMSProp (`lr=0.064 Ã— batch/256`, momentum=0.9, weight decay = 1e-5).  
- **Scheduler:** StepLR (decay = 0.96 every 8 epochs).  
- **Loss:** Cross-Entropy with optional `label_smoothing = 0.1`.  
- **Regularization:**  
  - Dropout (0.2 â€“ 0.5 depending on Ï†)  
  - Stochastic Depth (`drop_connect_rate` â‰ˆ 0.2 â€“ 0.5)  
  - BatchNorm (Îµ = 1e-3, momentum = 0.99)  
- **Mixed Precision:** enabled via `torch.amp.autocast()` + `GradScaler`.  
- **Resolution scaling:** handled automatically by `DynamicResize` using Î³^Ï†.

### Compound Scaling (Î±, Î², Î³)
| Model | Ï† | Depth (Î±^Ï†) | Width (Î²^Ï†) | Resolution (Î³^Ï†) | Drop-Connect |
|:------|--:|-------------:|-------------:|------------------:|--------------:|
| B0 | 0 | 1.0 | 1.0 | 1.0 | 0.2 |
| B1 | 1 | 1.1 | 1.1 | 1.15 | 0.2 |
| B2 | 2 | 1.2 | 1.1 | 1.15Â² | 0.3 |
| B4 | 4 | 1.4 | 1.2 | 1.15â´ | 0.4 |
| B7 | 7 | 2.0 | 1.4 | 1.15â· | 0.5 |

### Dataset
**Food-101**  
- 101 categories of dishes (~1000 images each).  
- Train = 75 750, Test = 25 250.  
- Augmentations: random crop 200Ã—200 â†’ resize Î³^Ï†, horizontal flip, color jitter.  
- Normalization: ImageNet mean & std.

### Stochastic Depth
Each residual block receives an individual survival probability:
$$
p_i = 1 - \text{drop\_connect\_rate} \times \frac{i}{N}
$$
ensuring early blocks survive almost always while deeper ones are randomly skipped.

## ğŸ¯ Educational Purpose

This project is built for **learning and experimentation**, not for production benchmarking.  
It demonstrates:

- How compound scaling couples **width, depth, and resolution**.  
- How to implement **MBConv + SE + Stochastic Depth** from scratch.  
- Proper **training loop design** with AMP, gradient clipping, and label smoothing.  
- How parameter efficiency can coexist with high representational power.

Each module is **unit-tested** in `tests/`, ensuring reliability and easier experimentation across model variants (B0â€“B7).

## ğŸ§¾ References

- Mingxing Tan & Quoc V. Le. *EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks.* ICML 2019.  
- Hu et al. *Squeeze-and-Excitation Networks.* CVPR 2018.  
- Sandler et al. *MobileNetV2: Inverted Residuals and Linear Bottlenecks.* CVPR 2018.

---

## âœï¸ Author

Developed by **Pablo Reyes**  
*Economist | Data Scientist | ML Researcher*  
ğŸ”— [github.com/pablo-reyes8](https://github.com/pablo-reyes8)
